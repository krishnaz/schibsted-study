{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! wget http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "! unzip ml-1m.zip -d ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratings = (pd.read_csv('./ml-1m/ratings.dat', engine='python', sep='::', names=['user', 'item', 'rating', 'timestamp'])\n",
    "    .assign(timestamp=lambda df:pd.to_datetime(df.timestamp * 1000000000))\n",
    "          )\n",
    "\n",
    "movies = (pd.read_csv('./ml-1m/movies.dat', engine='python', sep='::', names=['item', 'title', 'genres'])\n",
    "          .assign(genres=lambda df:df.genres.str.split('|').values)\n",
    "          .set_index('item', drop=False))\n",
    "\n",
    "# See http://files.grouplens.org/datasets/movielens/ml-1m-README.txt for more details\n",
    "users = pd.read_csv('./ml-1m/users.dat', engine='python', sep='::', \n",
    "                    names=['user', 'gender', 'age', 'occupation', 'zipcode'])\\\n",
    "    .set_index('user', drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>2000-12-31 22:12:40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>2000-12-31 22:35:09</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>2000-12-31 22:32:48</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>2000-12-31 22:04:35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>2001-01-06 23:38:11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item  rating           timestamp  feedback\n",
       "0     1  1193       5 2000-12-31 22:12:40         1\n",
       "1     1   661       3 2000-12-31 22:35:09        -1\n",
       "2     1   914       3 2000-12-31 22:32:48        -1\n",
       "3     1  3408       4 2000-12-31 22:04:35         1\n",
       "4     1  2355       5 2001-01-06 23:38:11         1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = ratings.assign(feedback=lambda df: 2 * (df.rating >= 4) - 1)\n",
    "\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split\n",
    "\n",
    " * Ideally time based split\n",
    " * For the sake of simplicity, let's just sample ratings uniformly (breaking the time machine rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900209, 5)\n",
      "(100000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>324271</th>\n",
       "      <td>1922</td>\n",
       "      <td>2094</td>\n",
       "      <td>4</td>\n",
       "      <td>2000-11-20 04:34:27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818637</th>\n",
       "      <td>4918</td>\n",
       "      <td>2808</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-07-08 19:29:05</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148677</th>\n",
       "      <td>957</td>\n",
       "      <td>1660</td>\n",
       "      <td>4</td>\n",
       "      <td>2000-11-25 05:28:13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778790</th>\n",
       "      <td>4653</td>\n",
       "      <td>914</td>\n",
       "      <td>5</td>\n",
       "      <td>2000-11-29 21:22:43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525489</th>\n",
       "      <td>3245</td>\n",
       "      <td>3324</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-09-07 06:33:31</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user  item  rating           timestamp  feedback\n",
       "324271  1922  2094       4 2000-11-20 04:34:27         1\n",
       "818637  4918  2808       1 2000-07-08 19:29:05        -1\n",
       "148677   957  1660       4 2000-11-25 05:28:13         1\n",
       "778790  4653   914       5 2000-11-29 21:22:43         1\n",
       "525489  3245  3324       1 2000-09-07 06:33:31        -1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ratings.sample(n=100000, random_state=0)\n",
    "train_ratings_mask = ~ratings.index.isin(test.index)\n",
    "train = ratings.loc[train_ratings_mask]\n",
    "\n",
    "test_user_items = test[['user', 'item']]\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refresher on cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entroy loss for softmax (multi class) classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10.,   1.,   1.],\n",
       "       [ 10.,   1.,   1.],\n",
       "       [  1.,   2.,   1.],\n",
       "       [  1.,   2.,   1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "N_CLASSES = 3\n",
    "\n",
    "logits_values = np.array([[10, 1, 1], [10, 1, 1], [1, 2, 1], [1, 2, 1]], dtype=float)\n",
    "logits = tf.constant(logits_values, shape=(BATCH_SIZE, N_CLASSES))\n",
    "\n",
    "logits.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class labels:\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [2]]\n",
      "one-hot encoded class labels:\n",
      "[[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "labels_values = np.array([0, 1, 1, 2])\n",
    "labels = tf.constant(labels_values)\n",
    "\n",
    "one_hot_values = np.array([[1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1]], dtype=np.float)\n",
    "one_hot_labels = tf.constant(one_hot_values)\n",
    "\n",
    "print('class labels:')\n",
    "print(tf.reshape(labels, (-1, 1)).eval())\n",
    "\n",
    "print('one-hot encoded class labels:')\n",
    "print(one_hot_labels.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.46789153e-04,   9.00024679e+00,   5.51444714e-01,\n",
       "         1.55144471e+00])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.nn.sparse_softmax_cross_entropy_with_logits (a la word2vec)\n",
    "# Measures the probability error in discrete classification tasks in which the\n",
    "# classes are mutually exclusive (each entry is in exactly one class).\n",
    "\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels)\n",
    "loss.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.99753241e-01,   1.23379352e-04,   1.23379352e-04],\n",
       "       [  9.99753241e-01,   1.23379352e-04,   1.23379352e-04],\n",
       "       [  2.11941558e-01,   5.76116885e-01,   2.11941558e-01],\n",
       "       [  2.11941558e-01,   5.76116885e-01,   2.11941558e-01]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step by step numpy equivalent\n",
    "raw_softmax_values = np.exp(logits_values)\n",
    "probabilities = (raw_softmax_values.T / np.sum(raw_softmax_values, axis=1)).T\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.46789153e-04,   9.00024679e+00,   5.51444714e-01,\n",
       "         1.55144471e+00])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_losses = -np.log(probabilities)[[0, 1, 2, 3], labels_values]\n",
    "cross_entropy_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.53988992e-05,   1.31326169e+00,   1.31326169e+00],\n",
       "       [  1.00000454e+01,   3.13261688e-01,   1.31326169e+00],\n",
       "       [  1.31326169e+00,   1.26928011e-01,   1.31326169e+00],\n",
       "       [  1.31326169e+00,   2.12692801e+00,   3.13261688e-01]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.nn.sigmoid_cross_entropy_with_logits (like in the CF auto-encoder article)\n",
    "# Measures the probability error in discrete classification tasks in which each\n",
    "# class is independent and not mutually exclusive.\n",
    "tf.nn.sigmoid_cross_entropy_with_logits(logits, targets=one_hot_labels).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9999546 ,  0.73105858,  0.73105858],\n",
       "       [ 0.9999546 ,  0.73105858,  0.73105858],\n",
       "       [ 0.73105858,  0.88079708,  0.73105858],\n",
       "       [ 0.73105858,  0.88079708,  0.73105858]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step by step numpy equivalent\n",
    "binary_probabilities = 1 / (1 + np.exp(-logits_values))\n",
    "binary_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.53988992e-05,   1.31326169e+00,   1.31326169e+00],\n",
       "       [  1.00000454e+01,   3.13261688e-01,   1.31326169e+00],\n",
       "       [  1.31326169e+00,   1.26928011e-01,   1.31326169e+00],\n",
       "       [  1.31326169e+00,   2.12692801e+00,   3.13261688e-01]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- (one_hot_values * np.log(binary_probabilities) + (1 - one_hot_values) * np.log(1 - binary_probabilities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow multi-class classification\n",
    "\n",
    "http://stackoverflow.com/questions/37671974/tensorflow-negative-sampling\n",
    "\n",
    "Can use:\n",
    " * `sparse_softmax_cross_entropy_with_logits` or `tf.nn.sampled_softmax_loss` (multi-class classsification a la word2vec)\n",
    " * but we could try [`tf.nn.sigmoid_cross_entropy_with_logits`](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#sigmoid_cross_entropy_with_logits) like in [Collaborative Denoising Auto-Encoders for Top-N Recommender Systems](http://yaowu.co/docs/wsdm16cdae.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6041, 3953)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "N_ITER = 100\n",
    "LOG_DIR = '/tmp/tflearn_logs'\n",
    "\n",
    "N_ITEMS = ratings.item.max() + 1\n",
    "N_USERS = ratings.user.max() + 1\n",
    "\n",
    "N_USERS, N_ITEMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on a topn metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>score</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1871</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>624</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>429</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>147</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>277</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item  score  rank\n",
       "0     1   1871     1\n",
       "1     2    624     2\n",
       "2     3    429     3\n",
       "3     4    147     4\n",
       "4     5    277     5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_by_items = train.groupby('item').size()\n",
    "item_count_ranks = (count_by_items\n",
    "                           .to_frame('score')\n",
    "                           .reset_index()\n",
    "                           .assign(rank=lambda df:np.arange(df.shape[0])+1))\n",
    "item_count_ranks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>score</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1871</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>624</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>429</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>147</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>277</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  item  score  rank\n",
       "0     0     1   1871     1\n",
       "1     0     2    624     2\n",
       "2     0     3    429     3\n",
       "3     0     4    147     4\n",
       "4     0     5    277     5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_ranked_user_item = (pd.merge(\n",
    "    left=pd.DataFrame.from_dict({'user': np.arange(N_USERS)}).assign(key=lambda df:np.ones_like(df.values)),\n",
    "    right=item_count_ranks.assign(key=lambda df:np.ones_like(df.item)),\n",
    "    on='key')\n",
    "        .drop(['key'], axis=1))\n",
    "\n",
    "pop_ranked_user_item.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.002738483542934887, 0.026426527520619424)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recall_precision_at_k(ranked_user_item, test, topk=1):\n",
    "    topk_ranked_user_item = ranked_user_item.query('rank <= {}'.format(topk))\n",
    "    in_test_mask = topk_ranked_user_item.user.isin(test.user.unique())\n",
    "    topk_in_test_ranked_user_item = topk_ranked_user_item[in_test_mask]\n",
    "\n",
    "    recs_and_test = pd.merge(\n",
    "        test,\n",
    "        topk_in_test_ranked_user_item,\n",
    "        on=['user', 'item'], how='outer')\n",
    "    \n",
    "    topk_predictions_mask = ~recs_and_test.score.isnull()\n",
    "    ground_truth_mask = recs_and_test.feedback == 1\n",
    "    n_hits = np.sum(ground_truth_mask & topk_predictions_mask).astype('float')\n",
    "    n_ground_truth = np.sum(ground_truth_mask)\n",
    "    n_predictions = np.sum(topk_predictions_mask)\n",
    "    return n_hits / n_ground_truth, n_hits / n_predictions\n",
    "\n",
    "recall_precision_at_k(pop_ranked_user_item, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 2.81 s per loop\n",
      "10 loops, best of 3: 46.1 ms per loop\n"
     ]
    }
   ],
   "source": [
    "# the auto-encoder net will produce scores for N_USERS x N_ITEMS \n",
    "all_user_item_scores = np.random.uniform(low=0, high=1, size=(N_USERS, N_ITEMS))\n",
    "\n",
    "\n",
    "def scores_to_frame(user_item_score_array, topk=1):\n",
    "    user_as_rows = np.arange(N_USERS)\n",
    "    item_as_cols = np.arange(N_ITEMS)\n",
    "\n",
    "    all_users_item_ranks = np.argsort(np.argsort(-user_item_score_array, axis=1), axis=1).ravel() + 1\n",
    "    only_topn_mask = all_users_item_ranks <= topk\n",
    "    return pd.DataFrame.from_dict({\n",
    "        'user': np.repeat(user_as_rows, N_ITEMS)[only_topn_mask],\n",
    "        'item': np.tile(item_as_cols, N_USERS)[only_topn_mask],\n",
    "        'score': all_users_item_ranks.ravel()[only_topn_mask],\n",
    "        'rank': all_users_item_ranks[only_topn_mask]\n",
    "    })\n",
    "\n",
    "def scores_to_top1_frame(user_item_score_array):\n",
    "    top1_items = np.argmax(user_item_score_array, axis=1)\n",
    "    user_as_rows = np.arange(N_USERS)\n",
    "    return pd.DataFrame.from_dict({\n",
    "        'user': user_as_rows,\n",
    "        'item': top1_items,\n",
    "        'score': user_item_score_array[user_as_rows,top1_items],\n",
    "        'rank': np.ones_like(top1_items)\n",
    "    })\n",
    "\n",
    "\n",
    "%timeit scores_to_frame(all_user_item_scores)\n",
    "\n",
    "%timeit scores_to_top1_frame(all_user_item_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  0: batch/test cross-entropy loss = 8.28/8.27\n",
      "         test recall/precision@1 = 0.0001/0.0008\n",
      "Step 10: batch/test cross-entropy loss = 8.20/8.19\n",
      "         test recall/precision@1 = 0.0021/0.0202\n",
      "Step 20: batch/test cross-entropy loss = 8.04/8.02\n",
      "         test recall/precision@1 = 0.0022/0.0215\n",
      "Step 30: batch/test cross-entropy loss = 7.74/7.71\n",
      "         test recall/precision@1 = 0.0034/0.0328\n",
      "Step 40: batch/test cross-entropy loss = 7.47/7.46\n",
      "         test recall/precision@1 = 0.0036/0.0343\n",
      "Step 50: batch/test cross-entropy loss = 7.38/7.38\n",
      "         test recall/precision@1 = 0.0044/0.0428\n",
      "Step 60: batch/test cross-entropy loss = 7.32/7.34\n",
      "         test recall/precision@1 = 0.0041/0.0392\n",
      "Step 70: batch/test cross-entropy loss = 7.29/7.31\n",
      "         test recall/precision@1 = 0.0042/0.0401\n",
      "Step 80: batch/test cross-entropy loss = 7.26/7.30\n",
      "         test recall/precision@1 = 0.0041/0.0396\n",
      "Step 90: batch/test cross-entropy loss = 7.24/7.29\n",
      "         test recall/precision@1 = 0.0043/0.0419\n"
     ]
    }
   ],
   "source": [
    "class UserAndItem2Vec:\n",
    "    def __init__(self, dimensionality=50, batch_size=None):\n",
    "        \n",
    "        with tf.name_scope('user_embeddings'):\n",
    "            user_embeddings = tf.Variable(tf.random_normal([N_USERS, dimensionality], \n",
    "                                                           stddev=0.01, mean=0), name='users')\n",
    "            tf.histogram_summary('user_embeddings', user_embeddings)\n",
    "\n",
    "        with tf.name_scope('item_biases'):\n",
    "            item_biases = tf.Variable(tf.random_normal([N_ITEMS], stddev=0.01, mean=0), name='items')\n",
    "\n",
    "        with tf.name_scope('item_embeddings'):\n",
    "            item_embeddings = tf.Variable(tf.random_normal([N_ITEMS, dimensionality], stddev=0.01, mean=0), name='items')\n",
    "            tf.histogram_summary('item_embeddings', item_embeddings)\n",
    "\n",
    "        self.user_embeddings = user_embeddings\n",
    "        self.item_embeddings = item_embeddings\n",
    "        self.item_biases = item_biases\n",
    "    \n",
    "        self.input_user_ids = tf.placeholder(\n",
    "            tf.int32, shape=[batch_size], name='user_ids')\n",
    "        self.input_positive_item_ids = tf.placeholder(\n",
    "            tf.int32, shape=[batch_size], name='positive_item_ids')\n",
    "\n",
    "    def user_to_all_item_logits(self):\n",
    "        \"\"\" This is the model described at equation (22) from http://yaowu.co/docs/wsdm16cdae.pdf\n",
    "        y_ui = \\sigma{W_i^t V_u + b_i}\n",
    "        The architecture is made of:\n",
    "         * input nodes for the user_id, and associated weights or embeddings V_u\n",
    "         * internal weights W_i and biases b_i for each items\n",
    "        \"\"\"\n",
    "        return tf.add(\n",
    "            self.item_biases, \n",
    "            tf.matmul(\n",
    "                tf.nn.embedding_lookup(self.user_embeddings, self.input_user_ids),\n",
    "                tf.transpose(self.item_embeddings)),\n",
    "            name='user_to_all_item_logits')\n",
    "\n",
    "    def sampled_loss(self):\n",
    "        with tf.name_scope('loss'):\n",
    "            sample_losses = tf.nn.sampled_softmax_loss(\n",
    "                biases=self.item_biases,\n",
    "                inputs=tf.nn.embedding_lookup(self.user_embeddings, self.input_user_ids),\n",
    "                labels=tf.reshape(self.input_positive_item_ids, (-1, 1)),\n",
    "                weights=self.item_embeddings,\n",
    "                num_classes=N_ITEMS,\n",
    "                num_sampled=10, num_true=1)\n",
    "            return tf.reduce_mean(sample_losses)\n",
    "        \n",
    "    def exact_loss(self):\n",
    "        with tf.name_scope('loss'):\n",
    "            cross_entropy_sum = tf.reduce_mean(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=self.user_to_all_item_logits(),\n",
    "                    labels=self.input_positive_item_ids))\n",
    "        return cross_entropy_sum\n",
    "\n",
    "def training(loss, learning_rate=0.01):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    return train_step\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    model = UserAndItem2Vec()\n",
    "    loss = model.exact_loss()    #.sampled_loss()\n",
    "    \n",
    "    tf.scalar_summary('batch_ll', loss)\n",
    "    summary = tf.merge_all_summaries()\n",
    "    test_summary = tf.scalar_summary('test_ll', loss)\n",
    "    train_step = training(loss)\n",
    "    \n",
    "    def perform_step(step, train, test, summary_writer):\n",
    "        positive_sample_ratings = train.query('feedback == 1').sample(BATCH_SIZE)\n",
    "        _, loss_value, summary_value = sess.run(\n",
    "            fetches=[train_step, loss, summary], \n",
    "            feed_dict={\n",
    "                model.input_user_ids: positive_sample_ratings.user.values,\n",
    "                model.input_positive_item_ids: positive_sample_ratings.item.values\n",
    "            })\n",
    "        summary_writer.add_summary(summary_value, global_step=step)\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            test_loss_value, test_summary_value = sess.run(\n",
    "                fetches=[loss, test_summary],\n",
    "                feed_dict={\n",
    "                    model.input_user_ids: test.query('feedback == 1').user.values,\n",
    "                    model.input_positive_item_ids: test.query('feedback == 1').item.values\n",
    "                })\n",
    "\n",
    "            all_users_item_logits = model.user_to_all_item_logits().eval(\n",
    "                feed_dict={model.input_user_ids: np.arange(N_USERS)})\n",
    "\n",
    "            print('Step {:2d}: batch/test cross-entropy loss = {:.2f}/{:.2f}'.format(step, loss_value, test_loss_value))\n",
    "            print('         test recall/precision@1 = {:.4f}/{:.4f}'.format(\n",
    "                *recall_precision_at_k(scores_to_top1_frame(all_users_item_logits), test)))\n",
    "            summary_writer.add_summary(test_summary_value, global_step=step)\n",
    "\n",
    "        summary_writer.flush()\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        summary_writer = tf.train.SummaryWriter(LOG_DIR + '/{:%Y%m%d%H%M%S}'.format(dt.datetime.now()), sess.graph)\n",
    "\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        for step in range(N_ITER):\n",
    "            perform_step(step, train, test, summary_writer)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.002738483542934887, 0.026426527520619424)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compared to pop \n",
    "recall_precision_at_k(pop_ranked_user_item, test)\n",
    "\n",
    "# In the collaborative auto-encoder paper they report a recall@1/map@1 on the movielense 10M of: \n",
    "# * (0.01, 0.19) for pop\n",
    "# * (0.04, 0.4) for their proposal\n",
    "# Can the fact that we use the ML 1M explain this difference?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
